import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.svm import SVC # Import SVM
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Sử dụng confusion_matrix của sklearn
import seaborn as sns
import joblib
import random
from scipy.interpolate import CubicSpline

# 1. Thu thập và chuẩn bị dữ liệu
# Thay đổi cấu trúc lưu trữ để theo dõi nguồn gốc file
data_with_origin = [] # Lưu tuple: (normalized_spectrum, raw_label, original_file_path)
folder_path = r'D:\Data\Fourier\Data_250509'
max_length = 0

def z_score_normalize(spectrum):
    """
    Chuẩn hóa phổ bằng phương pháp Z-score.
    """
    mean_val = np.mean(spectrum)
    std_dev = np.std(spectrum)
    if std_dev == 0:
        # Trả về mảng 0 nếu độ lệch chuẩn là 0 để tránh lỗi chia cho 0
        return np.zeros_like(spectrum)
    return (spectrum - mean_val) / std_dev

# --- HÀM AUGMENTATION MỚI: Baseline Correction Augmentation ---
def augment_spectrum_with_baseline(spectrum, num_control_points=4, max_amplitude=0.2):
    """
    Thêm một đường cong nền ngẫu nhiên vào phổ sử dụng Cubic Spline.
    """
    n = len(spectrum)
    x_points = np.linspace(0, n - 1, num_control_points)
    y_points = np.random.uniform(-max_amplitude, max_amplitude, num_control_points)
    spline = CubicSpline(x_points, y_points)
    baseline = spline(np.arange(n))
    augmented_spectrum = spectrum + baseline
    return augmented_spectrum

# --- HÀM AUGMENTATION SCALE
def augment_spectrum_scale(spectrum, scale_factor_range=(0.85, 1.15)):
    """
    Tăng cường phổ bằng cách nhân với một hệ số ngẫu nhiên.
    """
    scale_factor = np.random.uniform(scale_factor_range[0], scale_factor_range[1])
    return spectrum * scale_factor

# --- HÀM AUGMENTATION ADD NOISE
def augment_spectrum_with_noise(spectrum, noise_level=0.02):
    """
    Tăng cường phổ bằng cách thêm nhiễu ngẫu nhiên.
    """
    noise = np.random.normal(loc=0, scale=np.std(spectrum) * noise_level, size=spectrum.shape)
    return spectrum + noise


for folder_name in os.listdir(folder_path):
    sub_folder_path = os.path.join(folder_path, folder_name)
    if os.path.isdir(sub_folder_path):
        for file_name in os.listdir(sub_folder_path):
            if file_name.endswith('.dpt'):
                file_path = os.path.join(sub_folder_path, file_name)
                try:
                    spectrum = np.loadtxt(file_path, delimiter=',')
                    intensity = spectrum[:, 1]
                    wave = spectrum[:, 0]
                    length = 0
                    for i in range(0, len(wave)):
                        if wave[i] > 640:
                            length = length + 1
                    intensity_short = intensity[:length]

                    normalized_values = z_score_normalize(intensity_short)
                    raw_label = folder_name.split('_')

                    # --- THÊM DỮ LIỆU GỐC ---
                    data_with_origin.append((normalized_values, raw_label, file_path))
                    max_length = max(max_length, len(normalized_values))

                    # --- THÊM DỮ LIỆU ĐÃ TĂNG CƯỜNG ---
                    augmented_data_scale1 = augment_spectrum_scale(normalized_values, scale_factor_range=(0.9, 0.95))
                    data_with_origin.append((augmented_data_scale1, raw_label, file_path))

                    augmented_data_scale2 = augment_spectrum_scale(normalized_values, scale_factor_range=(1.05, 1.1))
                    data_with_origin.append((augmented_data_scale2, raw_label, file_path))

                    augmented_data_noise = augment_spectrum_with_noise(normalized_values, noise_level=0.02)
                    data_with_origin.append((augmented_data_noise, raw_label, file_path))

                    augmented_data_baseline = augment_spectrum_with_baseline(normalized_values, num_control_points=4, max_amplitude=0.2)
                    data_with_origin.append((augmented_data_baseline, raw_label, file_path))

                except Exception as e:
                    print(f"Lỗi khi đọc file {file_path}: {e}")

# Padding sequences to the maximum length (zero-padding ở cuối)
padded_data_with_origin = []
for _intensity, _label, _origin in data_with_origin:
    current_length = len(_intensity)
    if current_length < max_length:
        padding_length = max_length - current_length
        padded_intensity = np.pad(_intensity, (0, padding_length), 'constant', constant_values=0)
    else:
        padded_intensity = _intensity
    padded_data_with_origin.append((padded_intensity, _label, _origin))


# 2. Lọc dữ liệu cho 2 nhãn cụ thể và chuẩn bị cho SVM
# Các nhãn bạn muốn phân loại (có thể thay đổi tại đây)
TARGET_LABELS = ['PVDF', 'CInk']

filtered_X = []
filtered_y_raw = [] # Nhãn thô để binarize sau

for spectrum, raw_labels, _ in padded_data_with_origin:
    # Kiểm tra xem mẫu có chứa đúng một trong hai nhãn mục tiêu và không có nhãn nào khác
    # Đây là cách để đảm bảo chúng ta có các mẫu nhị phân rõ ràng
    is_target_sample = False
    current_target_label = None

    if TARGET_LABELS[0] in raw_labels and TARGET_LABELS[1] not in raw_labels:
        is_target_sample = True
        current_target_label = TARGET_LABELS[0]
    elif TARGET_LABELS[1] in raw_labels and TARGET_LABELS[0] not in raw_labels:
        is_target_sample = True
        current_target_label = TARGET_LABELS[1]

    if is_target_sample:
        filtered_X.append(spectrum)
        filtered_y_raw.append(current_target_label)

# Chuyển đổi dữ liệu thành numpy array
X = np.array(filtered_X)
y_raw = np.array(filtered_y_raw).reshape(-1, 1) # Reshape để MultiLabelBinarizer hoạt động

# Mã hóa nhãn cho 2 lớp
mlb_svm = MultiLabelBinarizer()
# Fit chỉ với các nhãn mục tiêu để đảm bảo thứ tự và mã hóa
mlb_svm.fit([TARGET_LABELS])
y = mlb_svm.transform(y_raw)
y = y.flatten() # Chuyển về 1D array cho SVM

# Lấy tên lớp sau khi mã hóa
unique_classes_svm = mlb_svm.classes_

print(f"\nĐang chuẩn bị dữ liệu cho phân loại '{TARGET_LABELS[0]}' và '{TARGET_LABELS[1]}'.")
print(f"Tổng số mẫu sau lọc: {len(X)}")
print(f"Độ dài vector đặc trưng: {X.shape[1]}")
print(f"Các lớp được mã hóa cho SVM: {unique_classes_svm}")


# Chia dữ liệu thành tập huấn luyện và tập kiểm tra (80/20)
# Sử dụng stratify để đảm bảo tỷ lệ lớp được giữ nguyên
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3. Xây dựng và huấn luyện mô hình SVM
# Kernel 'rbf' thường là một lựa chọn tốt cho nhiều bài toán
# C là tham số điều chỉnh lỗi phân loại trên tập huấn luyện (penalty parameter)
# gamma là tham số cho kernel 'rbf', ảnh hưởng đến phạm vi ảnh hưởng của một mẫu huấn luyện duy nhất
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
print("\nBắt đầu huấn luyện mô hình SVM...")
svm_model.fit(X_train, y_train)
print("Huấn luyện SVM hoàn tất.")

# 4. Đánh giá mô hình SVM
y_pred = svm_model.predict(X_test)

print("\nĐánh giá trên tập kiểm tra (SVM):")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=0, target_names=unique_classes_svm))

# Ma trận nhầm lẫn cho SVM
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=unique_classes_svm, yticklabels=unique_classes_svm)
plt.title('Confusion Matrix for SVM (PVDF vs CInk)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()


# Lưu mô hình SVM và MultiLabelBinarizer
joblib.dump(svm_model, 'svm_model_pvdf_cink.joblib')
joblib.dump(mlb_svm, 'mlb_svm_pvdf_cink.joblib')

print("\nMô hình SVM và MultiLabelBinarizer đã được lưu.")
